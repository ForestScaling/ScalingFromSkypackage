cbind(bootstrap_kde_log) %>%
dplyr::filter(. == TRUE)
if (nrow(peak_df) == 0) {
stop("No distinct peaks detected in the KDE curve for breakpoint identification.")
}
# Calculate the candidate breakpoint
potential_breakpoint <- peak_df %>%
dplyr::filter(log_x <= quantile(log10(size_vector_trimmed), 0.75)) %>%
dplyr::filter(log_x == max(log_x)) %>%
dplyr::pull(log_x)
# Handle cases where no breakpoint is found after filtering
if (length(potential_breakpoint) == 0) {
warning("No suitable breakpoint found after filtering peaks. Returning NA for breakpoint.")
potential_breakpoint <- NA
}
return(list(
potential_breakpoint = potential_breakpoint,
bootstrap_kde_log = bootstrap_kde_log,
original_data_trimmed = size_vector_trimmed, # Return the trimmed size vector
original_raw_data_df = original_raw_data_df, # Pass the original DF
trim_max_value = trim_max_value              # Pass trim_max
))
}
kde_results <- potential_break(
data = test_data$dbh,
n_bootstrap = 50,
bandwidth = "SJ",
trim_max = 50
)
library(dplyr)
kde_results <- potential_break(
data = test_data$dbh,
n_bootstrap = 50,
bandwidth = "SJ",
trim_max = 50
)
kde_results
# Run the second function
truncation_results <- truncate_filter(kde_results)
#'   It must contain `potential_breakpoint`, `bootstrap_kde_log`,
#'   `original_raw_data_df`, and `trim_max_value`.
#'
#' @return A list with:
#' \describe{
#'   \item{final_breakpoint}{The final determined lower log10 size bound.}
#'   \item{bayesian_data}{Subset of original data between 10^breakpoint and 10^upper_bound.}
#'   \item{kerneldens_logtransform}{The full log-scaled KDE data from the first function.}
#' }
#' @export
truncate_filter <- function(breakpoint_kde_results, min_size = 10) {
# Extract necessary components from the input list
potential_breakpoint <- breakpoint_kde_results$potential_breakpoint
bootstrap_kde_log <- breakpoint_kde_results$bootstrap_kde_log
original_data_trimmed_df <- data.frame(dbh=breakpoint_kde_results$original_data_trimmed)
trim_max <- breakpoint_kde_results$trim_max_value
# For robustness, let's ensure potential_breakpoint is not NA
if (is.na(potential_breakpoint)) {
stop("Potential breakpoint is NA. Cannot proceed with segmented regression. Check 'potential_break' output.")
}
# Filter the data for segmented regression *before* passing it to lm
bootstrap_kde_log_for_lm <- bootstrap_kde_log[bootstrap_kde_log$log_x >= potential_breakpoint, ]
# Ensure there's enough data for lm after filtering
if (nrow(bootstrap_kde_log_for_lm) < 2) { # Need at least 2 points for a line for lm
stop("Not enough data points after breakpoint filtering for segmented regression.")
}
# Segmented regression for upper truncation decision
lm_fit <- stats::lm(mean_log_density ~ log_x, data = bootstrap_kde_log_for_lm)
lm_fit$call$data<-bootstrap_kde_log_for_lm
seg_model <- segmented::selgmented(lm_fit,msg = FALSE)
breakpoints <- seg_model$psi[, "Est."]
slopes <- segmented::slope(seg_model)$log_x[, 1]
segment_boundaries <- c(min(bootstrap_kde_log$log_x), breakpoints, max(bootstrap_kde_log$log_x))
# Create segment data frame
segments_df <- data.frame(
left_x = head(segment_boundaries, -1),     # Left boundary of each segment
right_x = tail(segment_boundaries, -1),   # Right boundary of each segment
slope = slopes                    # Slope of each segment
)
if(segments_df[nrow(segments_df),]$slope<=-min_size | segments_df[nrow(segments_df),]$slope>0){
bayesian_data<-original_data_trimmed_df%>%
filter(dbh>=10^potential_breakpoint)%>%
filter(dbh>=min_size)%>%
filter(dbh<=10^segments_df[nrow(segments_df),]$left_x)
bootstrap_kde_log<-bootstrap_kde_log%>%
filter(log_x>=potential_breakpoint)%>%
filter(log_x>=log10(min_size))%>%
filter(log_x<=segments_df[nrow(segments_df),]$left_x)
}else{
bayesian_data<-original_data_trimmed_df%>%
filter(dbh>=10^potential_breakpoint)%>%
filter(dbh>=min_size)
bootstrap_kde_log<-bootstrap_kde_log%>%
filter(log_x>=potential_breakpoint)%>%
filter(log_x>=log10(min_size))
}
return(list(
bayesian_data = bayesian_data,
kerneldens_logtransform = bootstrap_kde_log, # Return the full KDE data for potential plotting
final_breakpoint = potential_breakpoint
))
}
# Run the second function
truncation_results <- truncate_filter(kde_results)
# Structure and type checks
expect_type(truncation_results, "list")
truncation_results
kde_results <- potential_break(
data = test_data$dbh,
n_bootstrap = 30,
bandwidth = "SJ",
trim_max = 50
)
truncation_results <- truncate_filter(kde_results, min_size = 15)
expect_true(all(truncation_results$bayesian_data$dbh >= 15))
library(testthat)
test_that("truncate_filter runs with valid input and returns expected structure", {
set.seed(42)
# Create synthetic data and run the first function
test_data <- data.frame(dbh = c(
rlnorm(500, meanlog = log(15), sdlog = 0.3),
rlnorm(500, meanlog = log(35), sdlog = 0.2)
))
kde_results <- potential_break(
data = test_data$dbh,
n_bootstrap = 50,
bandwidth = "SJ",
trim_max = 50
)
# Run the second function
truncation_results <- truncate_filter(kde_results)
# Structure and type checks
expect_type(truncation_results, "list")
expect_named(
truncation_results,
c("bayesian_data", "kerneldens_logtransform", "final_breakpoint")
)
expect_s3_class(truncation_results$bayesian_data, "data.frame")
expect_s3_class(truncation_results$kerneldens_logtransform, "data.frame")
expect_type(truncation_results$final_breakpoint, "double")
# Data sanity checks
expect_true(all(truncation_results$bayesian_data$dbh >= 10^truncation_results$final_breakpoint))
expect_true(all(truncation_results$kerneldens_logtransform$log_x >= truncation_results$final_breakpoint))
expect_true(all(is.finite(truncation_results$kerneldens_logtransform$mean_log_density)))
})
test_that("function stops with NA breakpoint", {
fake_kde_results <- list(
potential_breakpoint = NA,
bootstrap_kde_log = data.frame(log_x = 1:10, mean_log_density = rnorm(10)),
original_data_trimmed = runif(50, 10, 50),
trim_max_value = 50
)
expect_error(
truncate_filter(fake_kde_results),
"Potential breakpoint is NA"
)
})
test_that("function stops with insufficient data after filtering", {
fake_kde_results <- list(
potential_breakpoint = 2, # large value means little remaining data
bootstrap_kde_log = data.frame(
log_x = c(1.9, 1.95),  # only two points, right on the edge
mean_log_density = c(-2, -2.5)
),
original_data_trimmed = runif(100, 10, 50),
trim_max_value = 50
)
expect_error(
truncate_filter(fake_kde_results),
"Not enough data points after breakpoint filtering"
)
})
test_that("filtered data are within size limits and above min_size", {
set.seed(123)
test_data <- data.frame(dbh = rlnorm(300, meanlog = log(20), sdlog = 0.25))
kde_results <- potential_break(
data = test_data$dbh,
n_bootstrap = 30,
bandwidth = "SJ",
trim_max = 50
)
truncation_results <- truncate_filter(kde_results, min_size = 15)
expect_true(all(truncation_results$bayesian_data$dbh >= 15))
expect_true(all(truncation_results$kerneldens_logtransform$log_x >= log10(15)))
})
?head
dplyr::`%>%`
?dplyr::`%>%`
table
test
test_data
test_datadplyr::`%>%`mutate(test=1)
test_data dplyr::`%>%`mutate(test=1)
magrittr::`%>%`
??purrr
devtools::document()
`rm(list = c("potential_break", "truncate_filter"))`
rm(list = c("potential_break", "truncate_filter"))
devtools::document()
devtools::document()
?fit_alpha_model
devtools::document()
?truncate_filter
devtools::document()
?harv_data_sample
devtools::document()
devtools::document()
harv_data_sample
data("harv_data_sample"
)
harv_data_saple
harv_data_sample
save(harv_data_sample, file = "data/harv_data_sample.RData", version = 2)
devtools::document()
devtools::build()
devtools::check()
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log)
data<-harv_data_sample
data<-harv_data_sample$dbh
# Store original raw data for later use in the second function
original_raw_data_df <- data
trim_max_value <- trim_max # Store this as well
# Pull and trim size vector
size_vector_trimmed <- data[is.na(data) == FALSE]#[["dbh"]]
size_vector_trimmed <- size_vector_trimmed[size_vector_trimmed <= trim_max]
if (length(size_vector_trimmed) < 25) {
stop("Not enough observations below trim_max for breakpoint estimation (min 25 required).")
}
# KDE smoothing
kde <- stats::density(size_vector_trimmed, bw = bandwidth)
kde_df <- data.frame(x = kde$x, y = kde$y)
observed_vals <- sort(unique(size_vector_trimmed)) # Used for filtering KDE results
# filter KDE results to observed data range
kde_df <- kde_df %>%
dplyr::filter(x >= min(size_vector_trimmed), x <= max(size_vector_trimmed))
# Further  filter KDE to points near observed values
filtered_kde <- kde_df %>%
dplyr::rowwise() %>%
dplyr::filter(any(abs(x - observed_vals) <= 0.5)) %>%
dplyr::ungroup()
x_values <- filtered_kde$x
# Bootstrap KDEs
bootstrap_kdes <- replicate(
n_bootstrap,
stats::density(sample(size_vector_trimmed, length(size_vector_trimmed), replace = TRUE), bw = bandwidth),
simplify = FALSE
)
densities_matrix <- sapply(bootstrap_kdes, function(k) {
stats::approx(k$x, k$y, xout = x_values)$y
})
mean_density <- rowMeans(densities_matrix, na.rm = TRUE)
# Log-scaled kernel density data frame
bootstrap_kde_log <- data.frame(
log_x = log10(x_values),
mean_log_density = log10(mean_density)
)
trim_max = 50
n_bootstrap = 1000
bandwidth = "SJ"
# Store original raw data for later use in the second function
original_raw_data_df <- data
trim_max_value <- trim_max # Store this as well
# Pull and trim size vector
size_vector_trimmed <- data[is.na(data) == FALSE]#[["dbh"]]
size_vector_trimmed <- size_vector_trimmed[size_vector_trimmed <= trim_max]
if (length(size_vector_trimmed) < 25) {
stop("Not enough observations below trim_max for breakpoint estimation (min 25 required).")
}
# KDE smoothing
kde <- stats::density(size_vector_trimmed, bw = bandwidth)
kde_df <- data.frame(x = kde$x, y = kde$y)
observed_vals <- sort(unique(size_vector_trimmed)) # Used for filtering KDE results
# filter KDE results to observed data range
kde_df <- kde_df %>%
dplyr::filter(x >= min(size_vector_trimmed), x <= max(size_vector_trimmed))
# Further  filter KDE to points near observed values
filtered_kde <- kde_df %>%
dplyr::rowwise() %>%
dplyr::filter(any(abs(x - observed_vals) <= 0.5)) %>%
dplyr::ungroup()
x_values <- filtered_kde$x
# Bootstrap KDEs
bootstrap_kdes <- replicate(
n_bootstrap,
stats::density(sample(size_vector_trimmed, length(size_vector_trimmed), replace = TRUE), bw = bandwidth),
simplify = FALSE
)
densities_matrix <- sapply(bootstrap_kdes, function(k) {
stats::approx(k$x, k$y, xout = x_values)$y
})
mean_density <- rowMeans(densities_matrix, na.rm = TRUE)
# Log-scaled kernel density data frame
bootstrap_kde_log <- data.frame(
log_x = log10(x_values),
mean_log_density = log10(mean_density)
)
# Identify local peaks using splus2R::peaks()
peak_df <- splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log) %>%
dplyr::filter(. == TRUE)
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log)
devtools::document()
devtools::build()
devtools::check()
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log) %>%
dplyr::filter(.[[1]] == TRUE)
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log)
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log)$.
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log)
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log)%>%unique
t<-splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log)
t$.
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log) %>%
dplyr::filter(.[[2]] == TRUE)
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log) %>%
dplyr::filter(.[[3]] == TRUE)
splus2R::peaks(bootstrap_kde_log$mean_log_density) %>%
cbind(bootstrap_kde_log) %>%
dplyr::filter(.[[1]] == TRUE)
splus2R::peaks(bootstrap_kde_log$mean_log_density)
?cbind
bootstrap_kde_log <- cbind(
bootstrap_kde_log,
peaks = splus2R::peaks(bootstrap_kde_log$mean_log_density)
)
bootstrap_kde_log
# Identify local peaks using splus2R::peaks()
peak_df <- cbind(
bootstrap_kde_log,
peaks = splus2R::peaks(bootstrap_kde_log$mean_log_density)
)%>%
dplyr::filter(peaks == TRUE)
if (nrow(peak_df) == 0) {
stop("No distinct peaks detected in the KDE curve for breakpoint identification.")
}
# Identify local peaks using splus2R::peaks()
peak_df <- cbind(
bootstrap_kde_log,
peaks = splus2R::peaks(bootstrap_kde_log$mean_log_density)
)%>%
dplyr::filter(peaks == TRUE)
# Log-scaled kernel density data frame
bootstrap_kde_log <- data.frame(
log_x = log10(x_values),
mean_log_density = log10(mean_density)
)
# Identify local peaks using splus2R::peaks()
peak_df <- cbind(
bootstrap_kde_log,
peaks = splus2R::peaks(bootstrap_kde_log$mean_log_density))%>%
dplyr::filter(peaks == TRUE)
peak_df
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::check()
tools::resaveRdaFiles("data", compress = "xz")
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::check()
usethis::use_build_ignore(c("*.Rproj", ".Rproj.user"))
devtools::document()
usethis::use_build_ignore("ScalingFromSkypackage.Rproj")
devtools::document()
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::check()
use_mit_license()
usethis::`use_mit_license()`
usethis::use_mit_license()
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::check()
devtools::document()
devtools::build()
devtools::check()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::document()
devtools::build()
devtools::document()
devtools::check()
usethis::use_github_action("check-standard")
?use_github_action
usethis::use_github_action("check-standard")
HarvField<-read_sf("HARV_FieldData_ForestGEO_plotcorrected.shp")
library(sf)
HarvField<-read_sf("HARV_FieldData_ForestGEO_plotcorrected.shp")
getwd()
HarvField<-read_sf("C:\\Users\\adam.local\\Documents\\HARV_FieldData_ForestGEO_plotcorrected.shp")
HarvField
library(sf)
harvardshapefile <- sf::read_sf("C:/Users/adam.local/Documents/ForecastingScaling/harv_2014_weinstensegment_hectare.shp")
harvardshapefile
harvardshapefile<-harvardshapefile%>%select(-OBJECTID,-Join_Count,-TARGET_FID,-ID)
harvardshapefile<-harvardshapefile%>%select(-OBJECTID,-Join_Count,-TARGET_FID,-Id)
harvardshapefile
# Load the NEON Canopy Height Model (CHM) raster for the matching tile
# Keep the original file path exactly
chm <- rast("C:\\Users\\adam.local\\Downloads\\HARV_2014_canopyheight_chm\\NEON_struct-ecosystem\\NEON.D01.HARV.DP3.30015.001.2014-06.basic.20250723T152655Z.RELEASE-2025\\NEON_D01_HARV_DP3_731000_4713000_CHM.tif")
library(terra)
# Load the NEON Canopy Height Model (CHM) raster for the matching tile
# Keep the original file path exactly
chm <- rast("C:\\Users\\adam.local\\Downloads\\HARV_2014_canopyheight_chm\\NEON_struct-ecosystem\\NEON.D01.HARV.DP3.30015.001.2014-06.basic.20250723T152655Z.RELEASE-2025\\NEON_D01_HARV_DP3_731000_4713000_CHM.tif")
# Convert sf to terra's SpatVector for zonal extraction
harv1 <- vect(harvardshapefile)
# Compute maximum CHM (max canopy height) within each crown polygon
# na.rm = TRUE ensures NA pixels don't break the summary
zonalharv <- zonal(chm, harv1, fun = "max", na.rm = TRUE)
# Merge zonal stats (max height) back into the attribute table and rename
harvardshapefile <- cbind(
harv1 %>% data.frame(),
zonalharv %>%
rename(Max_Height = NEON_D01_HARV_DP3_731000_4713000_CHM)
)
harvardshapefile
harvardshapefile$Max_Height%>%summary
# Load the NEON Canopy Height Model (CHM) raster for the matching tile
# Keep the original file path exactly
chm <- rast("C:\\Users\\adam.local\\Downloads\\HARV_2014_canopyheight_chm\\NEON_struct-ecosystem\\NEON.D01.HARV.DP3.30015.001.2014-06.basic.20250723T152655Z.RELEASE-2025\\NEON_D01_HARV_DP3_731000_4713000_CHM.tif")
# Convert sf to terra's SpatVector for zonal extraction
harv1 <- vect(harvardshapefile)
harvardshapefile<-harvardshapefile%>%select(-OBJECTID,-Join_Count,-TARGET_FID,-Id)
library(sf)
harvardshapefile <- sf::read_sf("C:/Users/adam.local/Documents/ForecastingScaling/harv_2014_weinstensegment_hectare.shp")
harvardshapefile<-harvardshapefile%>%select(-OBJECTID,-Join_Count,-TARGET_FID,-Id)
library(terra)
library(dplyr)
library(sf)
# Load the NEON CHM raster
chm <- rast("C:\\Users\\adam.local\\Downloads\\HARV_2014_canopyheight_chm\\NEON_struct-ecosystem\\NEON.D01.HARV.DP3.30015.001.2014-06.basic.20250723T152655Z.RELEASE-2025\\NEON_D01_HARV_DP3_731000_4713000_CHM.tif")
# Convert sf object to SpatVector for zonal extraction
harv_vect <- vect(harvardshapefile)
# Extract max canopy height per polygon
zonalharv <- zonal(chm, harv_vect, fun = "max", na.rm = TRUE)
# Join results back to the original spatial data
# Note: zonalharv has an 'ID' column that matches polygon order
harv_vect$Max_Height <- zonalharv$max
# Convert back to sf object (optional but recommended for package data)
harvardshapefile <- st_as_sf(harv_vect)
# Inspect
print(harvardshapefile)
harvardshapefile
harv_vect
zonalharv
zonalharv%>%head()
?zonal
nrow(zonalharv)
nrow(harv1)
colname(zonalharv)
colnames\(zonalharv)
colnames(zonalharv)
library(terra)
library(dplyr)
library(sf)
# Load the NEON CHM raster
chm <- rast("C:\\Users\\adam.local\\Downloads\\HARV_2014_canopyheight_chm\\NEON_struct-ecosystem\\NEON.D01.HARV.DP3.30015.001.2014-06.basic.20250723T152655Z.RELEASE-2025\\NEON_D01_HARV_DP3_731000_4713000_CHM.tif")
# Convert sf object to SpatVector for zonal extraction
harv_vect <- vect(harvardshapefile)
# Extract max canopy height per polygon
zonalharv <- zonal(chm, harv_vect, fun = "max", na.rm = TRUE)
# Join results back to the original spatial data
# Note: zonalharv has an 'ID' column that matches polygon order
harv_vect$Max_Height <- zonalharv$NEON_D01_HARV_DP3_731000_4713000_CHM
# Convert back to sf object (optional but recommended for package data)
harvardshapefile <- st_as_sf(harv_vect)
# Inspect
print(harvardshapefile)
harvardshapefile%>%filter(is.nan(Max_Height)==FALSE)
harvardshapefile<-harvardshapefile%>%filter(is.nan(Max_Height)==FALSE)
usethis::use_data(harvardshapefile, compress = "xz")
head(harvardshapefile)
harvardshapefile<-harvardshapefile%>%select(-xmin,-ymin,-xmax,-ymax, label)
devtools::document()
?harvardshapefile
